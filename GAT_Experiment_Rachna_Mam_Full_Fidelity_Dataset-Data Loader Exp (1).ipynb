{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    # The classes must be sorted before encoding to enable static class encoding.\n",
    "    # In other words, make sure the first class always maps to index 0.\n",
    "    classes = sorted(list(set(labels)))\n",
    "    #print(\"encode_classes\",classes)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    #print(\"labels_onehot\",labels_onehot)\n",
    "    return labels_onehot,classes_dict\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    #print(\"pred\",preds.shape)\n",
    "    correct = preds.eq(labels).double()\n",
    "    #print(\"correct:\",correct.shape)\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adj and feature preparation from small fidelity dataset examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('VA_Final_Final_Sep08Ankit (Oct 8).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 2',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cogstore Mix Intent'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename({'Cogstore Mix Intent':'Label','Updated_Ankit':'Text'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FundAvailability</td>\n",
       "      <td>once i sell a portion of my roth how long will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PensionLetter</td>\n",
       "      <td>pension verification letter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HowDoImakeAstockOrMutualFundTrade</td>\n",
       "      <td>ok so i select exchange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FidelityHours</td>\n",
       "      <td>saturday hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ResearchInvestments</td>\n",
       "      <td>what companies grow cannabis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27722</th>\n",
       "      <td>UpdatePersonalInfo</td>\n",
       "      <td>i have changed my cell number and need to upda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27723</th>\n",
       "      <td>TakeOutLoan</td>\n",
       "      <td>if i pay off my 401 k loan am i immediately el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27724</th>\n",
       "      <td>StatementsViewOnline</td>\n",
       "      <td>how do i find year end statements for 2002 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27725</th>\n",
       "      <td>ContactFidelity</td>\n",
       "      <td>need to speak to representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27726</th>\n",
       "      <td>AcctRoutingNum</td>\n",
       "      <td>fidelity aba number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27727 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Label  \\\n",
       "0                       FundAvailability   \n",
       "1                          PensionLetter   \n",
       "2      HowDoImakeAstockOrMutualFundTrade   \n",
       "3                          FidelityHours   \n",
       "4                    ResearchInvestments   \n",
       "...                                  ...   \n",
       "27722                 UpdatePersonalInfo   \n",
       "27723                        TakeOutLoan   \n",
       "27724               StatementsViewOnline   \n",
       "27725                    ContactFidelity   \n",
       "27726                     AcctRoutingNum   \n",
       "\n",
       "                                                    Text  \n",
       "0      once i sell a portion of my roth how long will...  \n",
       "1                            pension verification letter  \n",
       "2                                ok so i select exchange  \n",
       "3                                         saturday hours  \n",
       "4                           what companies grow cannabis  \n",
       "...                                                  ...  \n",
       "27722  i have changed my cell number and need to upda...  \n",
       "27723  if i pay off my 401 k loan am i immediately el...  \n",
       "27724    how do i find year end statements for 2002 2006  \n",
       "27725                    need to speak to representative  \n",
       "27726                                fidelity aba number  \n",
       "\n",
       "[27727 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i want to boost my saving\n",
      "what is a unitized stock fund\n",
      "workplace disclaimer\n",
      "nointent\n"
     ]
    }
   ],
   "source": [
    "for inde,row in df.iterrows():\n",
    "    if row['Label']=='BoostSavings':\n",
    "        print(row['Text']) \n",
    "for inde,row in df.iterrows():\n",
    "    if row['Label']=='UnitizedStockFund':\n",
    "        print(row['Text'])\n",
    "for inde,row in df.iterrows():\n",
    "    if row['Label']=='WorkplaceDisclaimer':\n",
    "        print(row['Text'])\n",
    "for inde,row in df.iterrows():\n",
    "    if row['Label']=='NO_INTENT':\n",
    "        print(row['Text'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inde,row in df.iterrows():\n",
    "    if row['Label']=='NO_INTENT' or row['Label']=='UnitizedStockFund' or row['Label']=='WorkplaceDisclaimer' or row['Label']=='BoostSavings':\n",
    "        row['Label']='Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IwantToSetUpAtransfer        1166\n",
       "Contributions                1040\n",
       "IwantToSetUpAwithdrawal       813\n",
       "BillpayGeneral                810\n",
       "RolloverGeneral               809\n",
       "                             ... \n",
       "UsernameOverlap                 3\n",
       "GoalboosterNextSteps            3\n",
       "NetUnrealizedAppreciation       3\n",
       "WithdrawalDueToDisability       2\n",
       "ElectiveContribution            2\n",
       "Name: Label, Length: 190, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final DataStructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Text'].tolist()\n",
    "y=df['Label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First splitting then using Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.DataFrame()\n",
    "# df_train['Text'] = X_train\n",
    "# df_train['Label'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df =df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Data Loader for Making Dataset Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "dataset_size = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(Dataset):\n",
    "    def __init__(self,dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    def __getitem__(self,index):\n",
    "        return self.dataframe.iloc[index]['Text'],self.dataframe.iloc[index]['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PandasDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "c=0\n",
    "for index, row in df.iterrows():\n",
    "    if row['Label'] not in dic:\n",
    "        dic[row['Label']]=c\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_=[]\n",
    "for index,row in df.iterrows():\n",
    "    lis_.append(dic[row['Label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Labels']=lis_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FundAvailability</td>\n",
       "      <td>once i sell a portion of my roth how long will...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PensionLetter</td>\n",
       "      <td>pension verification letter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HowDoImakeAstockOrMutualFundTrade</td>\n",
       "      <td>ok so i select exchange</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FidelityHours</td>\n",
       "      <td>saturday hours</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ResearchInvestments</td>\n",
       "      <td>what companies grow cannabis</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27722</th>\n",
       "      <td>UpdatePersonalInfo</td>\n",
       "      <td>i have changed my cell number and need to upda...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27723</th>\n",
       "      <td>TakeOutLoan</td>\n",
       "      <td>if i pay off my 401 k loan am i immediately el...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27724</th>\n",
       "      <td>StatementsViewOnline</td>\n",
       "      <td>how do i find year end statements for 2002 2006</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27725</th>\n",
       "      <td>ContactFidelity</td>\n",
       "      <td>need to speak to representative</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27726</th>\n",
       "      <td>AcctRoutingNum</td>\n",
       "      <td>fidelity aba number</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27727 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Label  \\\n",
       "0                       FundAvailability   \n",
       "1                          PensionLetter   \n",
       "2      HowDoImakeAstockOrMutualFundTrade   \n",
       "3                          FidelityHours   \n",
       "4                    ResearchInvestments   \n",
       "...                                  ...   \n",
       "27722                 UpdatePersonalInfo   \n",
       "27723                        TakeOutLoan   \n",
       "27724               StatementsViewOnline   \n",
       "27725                    ContactFidelity   \n",
       "27726                     AcctRoutingNum   \n",
       "\n",
       "                                                    Text  Labels  \n",
       "0      once i sell a portion of my roth how long will...       0  \n",
       "1                            pension verification letter       1  \n",
       "2                                ok so i select exchange       2  \n",
       "3                                         saturday hours       3  \n",
       "4                           what companies grow cannabis       4  \n",
       "...                                                  ...     ...  \n",
       "27722  i have changed my cell number and need to upda...      94  \n",
       "27723  if i pay off my 401 k loan am i immediately el...      51  \n",
       "27724    how do i find year end statements for 2002 2006      18  \n",
       "27725                    need to speak to representative      15  \n",
       "27726                                fidelity aba number      33  \n",
       "\n",
       "[27727 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing all labels of data frame into a list named as y_train\n",
    "y_train = [row['Labels'] for index, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting count of each label\n",
    "l=[]\n",
    "for t in np.unique(y_train):\n",
    "    c=0\n",
    "    for j in y_train:\n",
    "        if t == j:\n",
    "            c+=1\n",
    "    l.append(c)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 1. / class_sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weight = 1. / class_sample_count\n",
    "#samples_weight = np.array([weight[v] for k,v in dic.items()])\n",
    "samples_weight = np.array([weight[t] for t in y_train])\n",
    "samples_weight = torch.from_numpy(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights = samples_weight,\n",
    "    num_samples=5000,\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4,sampler=weighted_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "it = iter(dataloader)\n",
    "#print(it)\n",
    "while True:\n",
    "    try:\n",
    "        x,y = next(it)\n",
    "        #print(x,y)\n",
    "        for item1,item2 in zip(x,y):\n",
    "            #print(item1)\n",
    "            X.append(item1)\n",
    "            Y.append(item2)\n",
    "    except StopIteration:\n",
    "        print(\"got\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame()\n",
    "dfs['Text']=X\n",
    "dfs['Label']=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dfs['Text'].tolist()\n",
    "y=dfs['Label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Text'] = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Label'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenePerStirpes                   27\n",
       "TaxesRetAcct                     27\n",
       "Medicare                         27\n",
       "ContactAnAdvisor                 26\n",
       "CobraQuestion                    25\n",
       "                                 ..\n",
       "IhaveAquestionAboutStateCodes    12\n",
       "RmdOverview                      11\n",
       "CaresWithdrawals                 10\n",
       "WithdrawalDueToDisability        10\n",
       "SecureMessageSend                 9\n",
       "Name: Label, Length: 190, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(df_train['Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenePerStirpes                   27\n",
       "TaxesRetAcct                     27\n",
       "Medicare                         27\n",
       "ContactAnAdvisor                 26\n",
       "CobraQuestion                    25\n",
       "                                 ..\n",
       "IhaveAquestionAboutStateCodes    12\n",
       "RmdOverview                      11\n",
       "CaresWithdrawals                 10\n",
       "WithdrawalDueToDisability        10\n",
       "SecureMessageSend                 9\n",
       "Name: Label, Length: 190, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.drop(df_train.index[df_train['Label'] == 'ElectiveContribution'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#df_train.drop(df_train.index[df_train['Label'] == 'WithdrawalDueToDisability'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=df_train['Text'].tolist()\n",
    "# y=df_train['Label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data frame Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_train['Text'] = X_train\n",
    "df_train['Label'] = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data frame Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Text'] = X_test\n",
    "df_test['Label'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3500"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) # Training Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test) # Test Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(df_test['Label'])) #Number of Unique Labels in test data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenePerStirpes            12\n",
       "TaxesRetAcct              12\n",
       "Medicare                  12\n",
       "TroubleWithWebsite        11\n",
       "FidelityMailingAddress    11\n",
       "                          ..\n",
       "PasswordStopSaving         5\n",
       "AcctClosure                5\n",
       "FsaTypes                   5\n",
       "CaresWithdrawals           4\n",
       "SecureMessageSend          4\n",
       "Name: Label, Length: 190, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenePerStirpes            12\n",
       "TaxesRetAcct              12\n",
       "Medicare                  12\n",
       "TroubleWithWebsite        11\n",
       "FidelityMailingAddress    11\n",
       "                          ..\n",
       "PasswordStopSaving         5\n",
       "AcctClosure                5\n",
       "FsaTypes                   5\n",
       "CaresWithdrawals           4\n",
       "SecureMessageSend          4\n",
       "Name: Label, Length: 190, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in range(len(X_train)):\n",
    "    l.append(1)\n",
    "for i in range(len(X_train),len(X)):\n",
    "    l.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train,df_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['train']=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3500\n",
       "0    1500\n",
       "Name: train, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['train'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_sentences, train_labels, test_sentences, test_labels = [],[],[],[]\n",
    "\n",
    "def get_dataset():\n",
    "    sentences = df['Text'].tolist()\n",
    "    labels = df['Label'].tolist()\n",
    "    train_or_test = df['train'].tolist()\n",
    "\n",
    "    original_train_size, original_test_size = 0, 0\n",
    "    for i in range(len(train_or_test)):\n",
    "        if train_or_test[i] == 1:\n",
    "            train_sentences.append(sentences[i])\n",
    "            train_labels.append(labels[i])\n",
    "            original_train_size += 1\n",
    "        elif train_or_test[i] == 0:\n",
    "            test_sentences.append(sentences[i])\n",
    "            test_labels.append(labels[i])\n",
    "            original_test_size += 1\n",
    "            \n",
    "    #print(\"There are\",len(df),\"samples in\",df)\n",
    "    print(\"Original Training set size:\",original_train_size)\n",
    "    print(\"Original Test set size:\",original_test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import scipy.sparse as sp\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Training set size: 3500\n",
      "Original Test set size: 1500\n"
     ]
    }
   ],
   "source": [
    "get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_comb=train_labels+test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import scipy.sparse as sp\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = encode_onehot(l_comb) # one hot vector for entire dataset representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenePerStirpes                   39\n",
       "TaxesRetAcct                     39\n",
       "Medicare                         39\n",
       "ContactAnAdvisor                 37\n",
       "PasswordRequirements             36\n",
       "                                 ..\n",
       "IhaveAquestionAboutStateCodes    17\n",
       "RmdOverview                      16\n",
       "WithdrawalDueToDisability        15\n",
       "CaresWithdrawals                 14\n",
       "SecureMessageSend                13\n",
       "Name: Label, Length: 190, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how do i update reinvest dividends and capital...</td>\n",
       "      <td>DripUpdate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why did i not receive a form 5498 for</td>\n",
       "      <td>TaxFormDidntReceive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mobile access</td>\n",
       "      <td>MobilePhoneNumber</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why isn't there an area marked tax forms</td>\n",
       "      <td>Taxforms</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how do i re balance my portfolio</td>\n",
       "      <td>RebalancePortfolio</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>where i have to upload a picture of my documen...</td>\n",
       "      <td>SecureMessageSend</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>ipad version of fidelity different</td>\n",
       "      <td>MobileApp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>where do i find the options referred to in the...</td>\n",
       "      <td>FullView</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>how much in taxes will it cost me to convert t...</td>\n",
       "      <td>RothConversionOverview</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>does the secure act affect my rmd</td>\n",
       "      <td>SecureAct</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0     how do i update reinvest dividends and capital...   \n",
       "1                 why did i not receive a form 5498 for   \n",
       "2                                         mobile access   \n",
       "3              why isn't there an area marked tax forms   \n",
       "4                      how do i re balance my portfolio   \n",
       "...                                                 ...   \n",
       "1495  where i have to upload a picture of my documen...   \n",
       "1496                 ipad version of fidelity different   \n",
       "1497  where do i find the options referred to in the...   \n",
       "1498  how much in taxes will it cost me to convert t...   \n",
       "1499                  does the secure act affect my rmd   \n",
       "\n",
       "                       Label  train  \n",
       "0                 DripUpdate      1  \n",
       "1        TaxFormDidntReceive      1  \n",
       "2          MobilePhoneNumber      1  \n",
       "3                   Taxforms      1  \n",
       "4         RebalancePortfolio      1  \n",
       "...                      ...    ...  \n",
       "1495       SecureMessageSend      0  \n",
       "1496               MobileApp      0  \n",
       "1497                FullView      0  \n",
       "1498  RothConversionOverview      0  \n",
       "1499               SecureAct      0  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adj and feature preparation from dummy examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import scipy.sparse as sp\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels,d = encode_onehot(l_comb) # one hot vector for entire dataset representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 190)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Adj and feature matrix accordance with GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(original_train_sentences, original_test_sentences):\n",
    "    #nltk.download('stopwords')\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    original_word_freq = {}  # to remove rare words\n",
    "    for sentence in original_train_sentences:\n",
    "        #temp = clean_str(sentence)\n",
    "        temp = sentence\n",
    "        word_list = temp.split()\n",
    "        for word in word_list:\n",
    "            if word in original_word_freq:\n",
    "                original_word_freq[word] += 1\n",
    "            else:\n",
    "                original_word_freq[word] = 1   \n",
    "\n",
    "    sentences = original_train_sentences + original_test_sentences\n",
    "    #print(sentences)\n",
    "    tokenize_sentences = []\n",
    "    word_list_dict = {}\n",
    "    for sentence in sentences:\n",
    "        #temp = clean_str(sentence)\n",
    "        temp = sentence\n",
    "        word_list_temp = temp.split()\n",
    "        doc_words = []\n",
    "        for word in word_list_temp:\n",
    "            #and word not in stop_words\n",
    "            if word in original_word_freq:\n",
    "                doc_words.append(word) # wo word company haijo test and train pr common word hai\n",
    "                word_list_dict[word] = 1\n",
    "        #print(doc_words)        \n",
    "        tokenize_sentences.append(doc_words) # doc words jo list hai usko append kr rha hu\n",
    "    word_list = list(word_list_dict.keys())\n",
    "    #print(\"tokenize sentences\",tokenize_sentences)\n",
    "    #print(word_list)\n",
    "\n",
    "    len_list = [len(l) for l in tokenize_sentences[:len(original_train_sentences)]]\n",
    "    print(\"Average Length:\", sum(len_list)/len(len_list))  \n",
    "    return tokenize_sentences, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length: 7.8262857142857145\n",
      "There are : 1968 unique words in total.\n"
     ]
    }
   ],
   "source": [
    "tokenize_sentences, word_list = preprocess_data(train_sentences, test_sentences)\n",
    "vocab_length = len(word_list)\n",
    "word_id_map = {}\n",
    "for i in range(vocab_length):\n",
    "    word_id_map[word_list[i]] = i\n",
    "#if not args.easy_copy:\n",
    "print(\"There are :\", vocab_length, \"unique words in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import scipy.sparse as sp\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def ordered_word_pair(a, b):\n",
    "    #print(a,b)\n",
    "    if a > b:\n",
    "        return b, a\n",
    "    else:\n",
    "        return a, b\n",
    "\n",
    "def get_adj(tokenize_sentences,train_size,word_id_map,word_list):\n",
    "    window_size = 20\n",
    "    total_W = 0\n",
    "    word_occurrence = {}\n",
    "    word_pair_occurrence = {}\n",
    "\n",
    "    node_size = train_size + len(word_list) #11 + 3\n",
    "    #print(\"node_size :\",node_size)\n",
    "    vocab_length = len(word_list)\n",
    "    #print(\"vocab_length :\",vocab_length) #11\n",
    "\n",
    "    def update_word_and_word_pair_occurrence(q):\n",
    "        #print(\"q :\",q)\n",
    "        unique_q = list(set(q))\n",
    "        for i in unique_q:\n",
    "            try:\n",
    "                word_occurrence[i] += 1\n",
    "            except:\n",
    "                word_occurrence[i] = 1\n",
    "        for i in range(len(unique_q)):\n",
    "            for j in range(i+1, len(unique_q)):\n",
    "                word1 = unique_q[i]\n",
    "                word2 = unique_q[j]\n",
    "                word1, word2 = ordered_word_pair(word1, word2)\n",
    "                try:\n",
    "                    word_pair_occurrence[(word1, word2)] += 1\n",
    "                except:\n",
    "                    word_pair_occurrence[(word1, word2)] = 1\n",
    "                    \n",
    "    for ind in range(train_size):\n",
    "        #print(\"iteration over training set :\",ind)\n",
    "        words = tokenize_sentences[ind]\n",
    "        #print(\"words :\",words)\n",
    "        q = []\n",
    "        for i in range(min(window_size, len(words))):\n",
    "            q += [word_id_map[words[i]]]\n",
    "            #print(\"q\",q)\n",
    "        total_W += 1\n",
    "        #print(\"total_W :\",total_W)\n",
    "        update_word_and_word_pair_occurrence(q)\n",
    "        now_next_word_index = window_size\n",
    "        #print(now_next_word_index)\n",
    "        while now_next_word_index<len(words):\n",
    "            q.pop(0)\n",
    "            q += [word_id_map[words[now_next_word_index]]]\n",
    "            now_next_word_index += 1\n",
    "            total_W += 1\n",
    "            update_word_and_word_pair_occurrence(q)\n",
    "    #print(len(word_pair_occurrence))\n",
    "    row = []\n",
    "    col = []\n",
    "    weight = []\n",
    "    for word_pair in word_pair_occurrence:\n",
    "        i = word_pair[0]\n",
    "        j = word_pair[1]\n",
    "        #print(\"word pair :\",i,j)\n",
    "        count = word_pair_occurrence[word_pair]\n",
    "        word_freq_i = word_occurrence[i]\n",
    "        word_freq_j = word_occurrence[j]\n",
    "        pmi = log((count * total_W) / (word_freq_i * word_freq_j)) \n",
    "        if pmi <=0:\n",
    "            continue\n",
    "        row.append(train_size + i)\n",
    "        col.append(train_size + j)\n",
    "        weight.append(pmi)\n",
    "        row.append(train_size + j)\n",
    "        col.append(train_size + i)\n",
    "        weight.append(pmi)\n",
    "    #print(\"Rows are \",row)\n",
    "    #print(\"Cols are \",col)\n",
    "\n",
    "\n",
    "    #print(\"weight :\",len(weight))\n",
    "    #print(\"row:\",len(row))\n",
    "    #print(\"col:\",col)\n",
    "    word_doc_list = {}\n",
    "    for word in word_list:\n",
    "        word_doc_list[word]=[]\n",
    "    #print(\"word_doc_list\",word_doc_list)\n",
    "    for i in range(train_size):\n",
    "        doc_words = tokenize_sentences[i]\n",
    "        #print(\"doc_words :\",doc_words)\n",
    "        unique_words = set(doc_words)\n",
    "        for word in unique_words:\n",
    "            exsit_list = word_doc_list[word]\n",
    "            exsit_list.append(i)\n",
    "            word_doc_list[word] = exsit_list\n",
    "            #print(\"word_doc_list\",word_doc_list)\n",
    "    word_doc_freq = {}\n",
    "    for word, doc_list in word_doc_list.items():\n",
    "        word_doc_freq[word] = len(doc_list)\n",
    "    #print(\"word_doc_freq :\",word_doc_freq)\n",
    "    doc_word_freq = {}\n",
    "\n",
    "    for doc_id in range(train_size):\n",
    "        words = tokenize_sentences[doc_id]\n",
    "        for word in words:\n",
    "            word_id = word_id_map[word]\n",
    "            doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "            if doc_word_str in doc_word_freq:\n",
    "                doc_word_freq[doc_word_str] += 1\n",
    "            else:\n",
    "                doc_word_freq[doc_word_str] = 1\n",
    "    #print(\"doc_word_freq :\",doc_word_freq)\n",
    "\n",
    "    doc_emb = np.zeros((train_size, vocab_length))\n",
    "\n",
    "    for i in range(train_size):\n",
    "        words = tokenize_sentences[i]\n",
    "        doc_word_set = set()\n",
    "        for word in words:\n",
    "            if word in doc_word_set:\n",
    "                continue\n",
    "            j = word_id_map[word]\n",
    "            key = str(i) + ',' + str(j)\n",
    "            freq = doc_word_freq[key]\n",
    "            row.append(i)\n",
    "            col.append(train_size + j)\n",
    "            idf = log(1.0 * train_size / word_doc_freq[word_list[j]])\n",
    "            w = freq * idf\n",
    "            weight.append(w)\n",
    "            #print(\"weight :\",weight)\n",
    "            doc_word_set.add(word)\n",
    "            doc_emb[i][j] = w/len(words)   \n",
    "            #print(doc_emb)\n",
    "        #print(len(doc_emb))\n",
    "    #print(len(wgt))\n",
    "    #wgt = weight \n",
    "    #print(weight)\n",
    "    adj = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)  \n",
    "    #print(\"adj\",adj)\n",
    "    return adj, doc_emb, word_doc_freq     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Graph\n",
    "adj, doc_emb, word_doc_freq = get_adj(tokenize_sentences,len(train_sentences)+len(test_sentences),word_id_map,word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6968, 6968)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1968)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo(), d_inv_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, norm_item = normalize_adj(adj + sp.eye(adj.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj = torch.FloatTensor(np.array(adj.todense())) #dense adjecency matric needed not sparse\n",
    "#features = torch.FloatTensor(np.array(features.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo=adj\n",
    "row = torch.from_numpy(coo.row.astype(np.int64)).to(torch.long)\n",
    "col = torch.from_numpy(coo.col.astype(np.int64)).to(torch.long)\n",
    "edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "#Presuming values are floats, can use np.int64 for dtype=int8\n",
    "val = torch.from_numpy(coo.data.astype(np.float64)).to(torch.float)\n",
    "\n",
    "out = torch.sparse.FloatTensor(edge_index, val, torch.Size(coo.shape)).to_dense() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6968, 6968])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1968"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape[0] - doc_emb.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1968)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13200 - 10500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "# weight = Parameter(torch.FloatTensor(2740, 2740))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support1 = torch.mm(torch.Tensor(doc_emb), weight)#(10500, 2740)*(2740 * 2740) = 10500*2740\n",
    "# support2 = weight #(2740 * 2740)\n",
    "# support = torch.vstack((support1,support2))#10500*2740+2740*2740 = 13200*2740\n",
    "# #support = dropout(support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition to previous Induct-Text GCN\n",
    "# l=[]\n",
    "# def Identity(size):\n",
    "#     for row in range(0, size):\n",
    "#         l1=[]\n",
    "#         for col in range(0, size):\n",
    "#             if (row == col):\n",
    "#                 l1.append(1)\n",
    "#             else:\n",
    "#                 l1.append(0)\n",
    "#         l.append((l1))\n",
    " \n",
    "# # Driver Code       \n",
    "# size = adj.shape[0] - doc_emb.shape[0]\n",
    "# Identity(size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = doc_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d=np.concatenate((d, np.array(l)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_emb = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.FloatTensor(doc_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1968])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.LongTensor(np.where(labels)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 47, 159, 102,  ...,  64, 135, 140])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5000, 1968]), torch.Size([6968, 6968]), torch.Size([5000]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape,adj.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_val(train_size, train_pro=0.9):\n",
    "    real_train_size = int(train_pro*train_size)\n",
    "    val_size = train_size-real_train_size\n",
    "\n",
    "    #if args.shuffle_seed!=None:\n",
    "    #     np.random.seed()\n",
    "    idx_train = np.random.choice(train_size, real_train_size,replace=False)\n",
    "    idx_train.sort()\n",
    "    idx_val = []\n",
    "    pointer = 0\n",
    "    for v in range(train_size):\n",
    "        if pointer<len(idx_train) and idx_train[pointer] == v:\n",
    "            pointer +=1\n",
    "        else:\n",
    "            #print(v)\n",
    "            idx_val.append(v)\n",
    "    return idx_train, idx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train/val dataset\n",
    "idx_train, idx_val = generate_train_val(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-defining train test split for dummy data\n",
    "#idx_train = list(range(len(X_train)-800))\n",
    "#idx_val = list(range(len(X_train)-800, len(X_train)))\n",
    "idx_test = list(range(len(X_train),len(X)))\n",
    "#print(\"index of train\",idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3500, 3501, 3502,  ..., 4997, 4998, 4999])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1968"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape[1] # Vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3150"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphAttentionLayer\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "    print(\"GraphAttentionLayer\")\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)#10*16\n",
    "        support1 = Wh\n",
    "        if self.concat:\n",
    "            support2 = self.W\n",
    "            support = torch.vstack((support1,support2))\n",
    "            #support = self.dropout(support)\n",
    "            support = F.dropout(support, self.dropout, training=self.training)\n",
    "            Wh = support\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "        #print(\"E \",e.shape)\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        #print(\"Adj shape\",adj.shape)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)#10*10 * 10*16=10*16\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        # Wh.shape (N, out_feature)\n",
    "        # self.a.shape (2 * out_feature, 1)\n",
    "        # Wh1&2.shape (N, 1)\n",
    "        # e.shape (N, N)\n",
    "        #print(\"Inside prepare_attention_mechanism_calculation\",self.out_features)\n",
    "        #print(\"a shape with out feature self.a[:self.out_features, :]\",self.a[:self.out_features, :].shape)\n",
    "        #print(\"a shape with self.a[self.out_features:, :]\",self.a[self.out_features:, :].shape)\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
    "        # broadcast add\n",
    "        e = Wh1 + Wh2.T\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "        self.nheads=nheads\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "       \n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        #print(\"Adding x \",x.shape)\n",
    "        self.hid = x\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT CALL\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "random.seed(72)\n",
    "np.random.seed(72)\n",
    "torch.manual_seed(72)\n",
    "print(\"GAT CALL\")\n",
    "model = GAT(nfeat=features.shape[1], \n",
    "                nhid=128, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=0.1, \n",
    "                nheads=12, \n",
    "                alpha=0.2)\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.002, \n",
    "                       weight_decay=0)\n",
    "\n",
    "\n",
    "features, adj, labels = Variable(features), Variable(torch.FloatTensor(adj)), Variable(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Dummy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0001 loss_train: 5.2481 acc_train: 0.0035 loss_val: 5.2478 acc_val: 0.0000 time: 5.7554s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0002 loss_train: 5.1843 acc_train: 0.2743 loss_val: 5.1947 acc_val: 0.2200 time: 5.7282s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0003 loss_train: 5.1209 acc_train: 0.5187 loss_val: 5.1479 acc_val: 0.3914 time: 5.7200s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0004 loss_train: 5.0508 acc_train: 0.5978 loss_val: 5.0930 acc_val: 0.4429 time: 5.7151s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0005 loss_train: 4.9840 acc_train: 0.6270 loss_val: 5.0359 acc_val: 0.4771 time: 5.7078s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0006 loss_train: 4.9189 acc_train: 0.6362 loss_val: 4.9931 acc_val: 0.4714 time: 5.7266s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0007 loss_train: 4.8273 acc_train: 0.6352 loss_val: 4.9098 acc_val: 0.4857 time: 5.7309s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0008 loss_train: 4.7455 acc_train: 0.6359 loss_val: 4.8401 acc_val: 0.4914 time: 5.7202s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0009 loss_train: 4.6410 acc_train: 0.6356 loss_val: 4.7619 acc_val: 0.5029 time: 5.7283s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0010 loss_train: 4.5480 acc_train: 0.6317 loss_val: 4.6903 acc_val: 0.4886 time: 5.7068s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0011 loss_train: 4.4113 acc_train: 0.6248 loss_val: 4.5837 acc_val: 0.4829 time: 5.7220s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0012 loss_train: 4.2958 acc_train: 0.6098 loss_val: 4.4963 acc_val: 0.4914 time: 5.7203s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0013 loss_train: 4.1527 acc_train: 0.5889 loss_val: 4.3793 acc_val: 0.4829 time: 5.7412s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0014 loss_train: 4.0340 acc_train: 0.5794 loss_val: 4.2975 acc_val: 0.4457 time: 5.7360s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0015 loss_train: 3.9059 acc_train: 0.5692 loss_val: 4.2325 acc_val: 0.4400 time: 5.7219s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0016 loss_train: 3.7930 acc_train: 0.5644 loss_val: 4.0479 acc_val: 0.4571 time: 5.7141s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0017 loss_train: 3.6632 acc_train: 0.5622 loss_val: 4.0125 acc_val: 0.4371 time: 5.7275s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0018 loss_train: 3.5683 acc_train: 0.5610 loss_val: 3.9344 acc_val: 0.4400 time: 5.7165s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0019 loss_train: 3.4422 acc_train: 0.5603 loss_val: 3.8611 acc_val: 0.4371 time: 5.7340s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0020 loss_train: 3.3115 acc_train: 0.5698 loss_val: 3.6791 acc_val: 0.4457 time: 5.7275s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0021 loss_train: 3.2280 acc_train: 0.5717 loss_val: 3.6601 acc_val: 0.4543 time: 5.7257s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0022 loss_train: 3.1258 acc_train: 0.5708 loss_val: 3.5055 acc_val: 0.4629 time: 5.7361s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0023 loss_train: 2.9953 acc_train: 0.5743 loss_val: 3.4452 acc_val: 0.4429 time: 5.7406s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0024 loss_train: 2.9163 acc_train: 0.5768 loss_val: 3.3832 acc_val: 0.4800 time: 5.7291s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0025 loss_train: 2.8309 acc_train: 0.5832 loss_val: 3.4090 acc_val: 0.4629 time: 5.7264s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0026 loss_train: 2.7185 acc_train: 0.5940 loss_val: 3.3259 acc_val: 0.4600 time: 6.1853s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0027 loss_train: 2.6244 acc_train: 0.6006 loss_val: 3.2154 acc_val: 0.4829 time: 6.7657s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0028 loss_train: 2.5801 acc_train: 0.5908 loss_val: 3.2056 acc_val: 0.4629 time: 5.7283s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0029 loss_train: 2.4788 acc_train: 0.6057 loss_val: 3.1568 acc_val: 0.4800 time: 5.7090s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0030 loss_train: 2.3879 acc_train: 0.6181 loss_val: 3.0689 acc_val: 0.4886 time: 5.7082s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0031 loss_train: 2.3390 acc_train: 0.6152 loss_val: 3.0115 acc_val: 0.4886 time: 5.7209s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0032 loss_train: 2.2636 acc_train: 0.6286 loss_val: 2.9296 acc_val: 0.4800 time: 5.7145s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0033 loss_train: 2.2202 acc_train: 0.6356 loss_val: 2.8927 acc_val: 0.5057 time: 5.7125s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0034 loss_train: 2.1621 acc_train: 0.6403 loss_val: 2.8241 acc_val: 0.5086 time: 5.7185s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0035 loss_train: 2.0800 acc_train: 0.6590 loss_val: 2.6355 acc_val: 0.5457 time: 5.7187s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0036 loss_train: 2.0203 acc_train: 0.6549 loss_val: 2.8127 acc_val: 0.5171 time: 5.7137s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0037 loss_train: 1.9463 acc_train: 0.6724 loss_val: 2.6842 acc_val: 0.4971 time: 5.7300s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0038 loss_train: 1.8938 acc_train: 0.6638 loss_val: 2.5999 acc_val: 0.5143 time: 5.7271s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0039 loss_train: 1.8067 acc_train: 0.6717 loss_val: 2.6487 acc_val: 0.5029 time: 5.7213s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0040 loss_train: 1.7895 acc_train: 0.6819 loss_val: 2.5741 acc_val: 0.5229 time: 5.7272s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0041 loss_train: 1.7729 acc_train: 0.6778 loss_val: 2.6231 acc_val: 0.5371 time: 5.7255s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0042 loss_train: 1.6959 acc_train: 0.6943 loss_val: 2.4253 acc_val: 0.5457 time: 5.7263s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0043 loss_train: 1.6025 acc_train: 0.6994 loss_val: 2.4783 acc_val: 0.5429 time: 5.7341s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0044 loss_train: 1.5871 acc_train: 0.7149 loss_val: 2.4645 acc_val: 0.5829 time: 5.7259s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0045 loss_train: 1.4987 acc_train: 0.7197 loss_val: 2.4841 acc_val: 0.5629 time: 5.7244s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0046 loss_train: 1.4971 acc_train: 0.7260 loss_val: 2.3597 acc_val: 0.5686 time: 5.7333s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0047 loss_train: 1.4712 acc_train: 0.7254 loss_val: 2.2622 acc_val: 0.5543 time: 5.7298s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0048 loss_train: 1.3810 acc_train: 0.7454 loss_val: 2.3332 acc_val: 0.5886 time: 5.7342s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0049 loss_train: 1.3953 acc_train: 0.7454 loss_val: 2.3476 acc_val: 0.5714 time: 5.7266s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0050 loss_train: 1.3850 acc_train: 0.7489 loss_val: 2.1984 acc_val: 0.6086 time: 5.7274s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0051 loss_train: 1.3399 acc_train: 0.7486 loss_val: 2.2858 acc_val: 0.5800 time: 5.7258s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0052 loss_train: 1.3049 acc_train: 0.7502 loss_val: 2.2189 acc_val: 0.5943 time: 5.7277s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0053 loss_train: 1.2970 acc_train: 0.7737 loss_val: 2.2829 acc_val: 0.6200 time: 5.7287s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0054 loss_train: 1.2261 acc_train: 0.7683 loss_val: 2.2457 acc_val: 0.6229 time: 5.7404s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0055 loss_train: 1.2236 acc_train: 0.7759 loss_val: 2.1070 acc_val: 0.6200 time: 5.7379s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0056 loss_train: 1.1919 acc_train: 0.7844 loss_val: 2.0435 acc_val: 0.6257 time: 5.7296s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0057 loss_train: 1.1572 acc_train: 0.7822 loss_val: 2.1668 acc_val: 0.6171 time: 5.9712s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0058 loss_train: 1.0897 acc_train: 0.7937 loss_val: 2.1047 acc_val: 0.6143 time: 6.9380s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0059 loss_train: 1.0606 acc_train: 0.8032 loss_val: 2.1061 acc_val: 0.6057 time: 5.7406s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0060 loss_train: 1.0057 acc_train: 0.8143 loss_val: 2.1257 acc_val: 0.6429 time: 5.7263s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0061 loss_train: 1.0098 acc_train: 0.8130 loss_val: 2.0542 acc_val: 0.6314 time: 5.7205s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0062 loss_train: 1.0252 acc_train: 0.8130 loss_val: 2.0785 acc_val: 0.6457 time: 5.7233s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0063 loss_train: 0.9815 acc_train: 0.8187 loss_val: 2.0463 acc_val: 0.6400 time: 5.7329s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0064 loss_train: 0.9734 acc_train: 0.8244 loss_val: 2.1367 acc_val: 0.6371 time: 5.7256s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0065 loss_train: 0.8865 acc_train: 0.8432 loss_val: 2.0515 acc_val: 0.6629 time: 5.7217s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0066 loss_train: 0.8555 acc_train: 0.8562 loss_val: 2.0003 acc_val: 0.6600 time: 5.7224s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0067 loss_train: 0.8856 acc_train: 0.8514 loss_val: 1.9993 acc_val: 0.6857 time: 5.7289s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0068 loss_train: 0.8642 acc_train: 0.8578 loss_val: 2.0359 acc_val: 0.6429 time: 5.7184s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0069 loss_train: 0.8162 acc_train: 0.8654 loss_val: 1.8731 acc_val: 0.6771 time: 5.7332s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0070 loss_train: 0.8077 acc_train: 0.8594 loss_val: 1.9466 acc_val: 0.6686 time: 5.7270s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0071 loss_train: 0.7816 acc_train: 0.8632 loss_val: 1.9762 acc_val: 0.6714 time: 5.7293s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0072 loss_train: 0.7623 acc_train: 0.8724 loss_val: 1.8755 acc_val: 0.6857 time: 5.7253s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0073 loss_train: 0.7011 acc_train: 0.8835 loss_val: 1.7926 acc_val: 0.7057 time: 5.7300s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0074 loss_train: 0.6986 acc_train: 0.8800 loss_val: 1.7748 acc_val: 0.6914 time: 5.7406s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0075 loss_train: 0.7291 acc_train: 0.8860 loss_val: 1.6698 acc_val: 0.6829 time: 5.7349s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0076 loss_train: 0.6748 acc_train: 0.8816 loss_val: 1.7689 acc_val: 0.6829 time: 5.7275s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0077 loss_train: 0.6514 acc_train: 0.8794 loss_val: 1.7872 acc_val: 0.7029 time: 5.7257s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0078 loss_train: 0.5720 acc_train: 0.8914 loss_val: 1.6767 acc_val: 0.7171 time: 5.7263s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0079 loss_train: 0.6056 acc_train: 0.8962 loss_val: 1.6774 acc_val: 0.7114 time: 5.7317s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0080 loss_train: 0.5776 acc_train: 0.8994 loss_val: 1.7503 acc_val: 0.6886 time: 5.7347s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0081 loss_train: 0.5630 acc_train: 0.9063 loss_val: 1.7239 acc_val: 0.6943 time: 5.7285s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0082 loss_train: 0.5347 acc_train: 0.9114 loss_val: 1.6453 acc_val: 0.7086 time: 5.7282s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0083 loss_train: 0.5318 acc_train: 0.9038 loss_val: 1.7703 acc_val: 0.6943 time: 5.7321s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0084 loss_train: 0.5151 acc_train: 0.9079 loss_val: 1.6336 acc_val: 0.7257 time: 5.7227s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0085 loss_train: 0.4887 acc_train: 0.9102 loss_val: 1.6989 acc_val: 0.7086 time: 5.7388s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0086 loss_train: 0.4705 acc_train: 0.9248 loss_val: 1.6038 acc_val: 0.7143 time: 5.7394s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0087 loss_train: 0.4578 acc_train: 0.9254 loss_val: 1.6886 acc_val: 0.7086 time: 5.7261s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0088 loss_train: 0.4728 acc_train: 0.9165 loss_val: 1.6221 acc_val: 0.6943 time: 5.7460s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0089 loss_train: 0.4300 acc_train: 0.9187 loss_val: 1.6202 acc_val: 0.7086 time: 7.1121s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0090 loss_train: 0.4316 acc_train: 0.9210 loss_val: 1.6028 acc_val: 0.7229 time: 5.8765s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0091 loss_train: 0.4359 acc_train: 0.9298 loss_val: 1.6134 acc_val: 0.7200 time: 5.7256s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0092 loss_train: 0.4204 acc_train: 0.9260 loss_val: 1.6418 acc_val: 0.7171 time: 5.7306s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0093 loss_train: 0.3698 acc_train: 0.9413 loss_val: 1.6513 acc_val: 0.7200 time: 5.7304s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0094 loss_train: 0.3647 acc_train: 0.9387 loss_val: 1.4704 acc_val: 0.7286 time: 5.7305s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0095 loss_train: 0.3699 acc_train: 0.9368 loss_val: 1.6091 acc_val: 0.7257 time: 5.7312s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0096 loss_train: 0.3793 acc_train: 0.9365 loss_val: 1.4693 acc_val: 0.7400 time: 5.7381s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0097 loss_train: 0.3856 acc_train: 0.9352 loss_val: 1.5777 acc_val: 0.7086 time: 5.7262s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0098 loss_train: 0.3579 acc_train: 0.9359 loss_val: 1.6303 acc_val: 0.7343 time: 5.7233s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0099 loss_train: 0.3584 acc_train: 0.9330 loss_val: 1.5698 acc_val: 0.6971 time: 5.7291s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0100 loss_train: 0.3415 acc_train: 0.9413 loss_val: 1.5480 acc_val: 0.7257 time: 5.7370s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0101 loss_train: 0.3498 acc_train: 0.9394 loss_val: 1.5102 acc_val: 0.7257 time: 5.7268s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0102 loss_train: 0.3210 acc_train: 0.9486 loss_val: 1.6501 acc_val: 0.6857 time: 5.7297s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0103 loss_train: 0.3342 acc_train: 0.9362 loss_val: 1.5872 acc_val: 0.7429 time: 5.7336s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0104 loss_train: 0.2922 acc_train: 0.9476 loss_val: 1.5900 acc_val: 0.6943 time: 5.7249s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0105 loss_train: 0.2709 acc_train: 0.9559 loss_val: 1.5528 acc_val: 0.7429 time: 5.7262s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0106 loss_train: 0.2915 acc_train: 0.9508 loss_val: 1.5806 acc_val: 0.7200 time: 5.7365s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0107 loss_train: 0.2824 acc_train: 0.9517 loss_val: 1.5049 acc_val: 0.7314 time: 5.7333s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0108 loss_train: 0.2508 acc_train: 0.9517 loss_val: 1.6802 acc_val: 0.7343 time: 5.7295s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0109 loss_train: 0.2455 acc_train: 0.9610 loss_val: 1.5168 acc_val: 0.7086 time: 5.7326s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0110 loss_train: 0.2654 acc_train: 0.9594 loss_val: 1.5710 acc_val: 0.7314 time: 5.7284s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0111 loss_train: 0.2627 acc_train: 0.9559 loss_val: 1.5606 acc_val: 0.7057 time: 5.7451s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0112 loss_train: 0.2683 acc_train: 0.9530 loss_val: 1.6051 acc_val: 0.7286 time: 5.7425s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0113 loss_train: 0.2591 acc_train: 0.9514 loss_val: 1.6293 acc_val: 0.7143 time: 5.7259s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0114 loss_train: 0.2546 acc_train: 0.9524 loss_val: 1.5880 acc_val: 0.7114 time: 5.7454s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0115 loss_train: 0.2452 acc_train: 0.9568 loss_val: 1.6147 acc_val: 0.7171 time: 5.7309s\n",
      "model call torch.Size([5000, 1968]) torch.Size([6968, 6968])\n",
      "Epoch: 0116 loss_train: 0.2671 acc_train: 0.9565 loss_val: 1.5351 acc_val: 0.7229 time: 5.7399s\n",
      "Early Stopping...\n",
      "Optimization Finished!\n",
      "Total time elapsed: 669.0132s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from utils import cal_accuracy\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "val_loss = []\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    #print(\"model train\")\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"model call\",features.shape,adj.shape)\n",
    "    output = model(features, adj)\n",
    "    #print(\"output\",output)\n",
    "    #print(\"loss calculation\")\n",
    "    #print(\"output[idx_train]\",output[idx_train],labels[idx_train])\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    val_loss.append(loss_val.item())\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    train_losses.append(loss_train.item())\n",
    "    val_losses.append(loss_val.item())\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.data.item()\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "# bad_counter = 0\n",
    "# best = 10 + 1\n",
    "# best_epoch = 0\n",
    "for epoch in range(200):\n",
    "    loss_values.append(train(epoch))\n",
    "    if epoch > 20 and np.min(val_loss[-20:]) > np.min(val_loss[:-20]) :\n",
    "            print(\"Early Stopping...\")\n",
    "            break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.plot(val_losses,label=\"val\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_hidden_local(model,features,adj):\n",
    "    model.eval()\n",
    "    nheads = model.nheads\n",
    "    gat_layer1_w = [model.attentions[i].W for i in range(nheads)]\n",
    "    gat_layer1_a = [model.attentions[i].a for i in range(nheads)]\n",
    "    gat_layer2_w = model.out_att.W\n",
    "    gat_layer2_a = model.out_att.a\n",
    "    layer1_hidden_weights = model.hid\n",
    "    return [gat_layer1_w,gat_layer1_a,layer1_hidden_weights,gat_layer2_w,gat_layer2_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_list = get_weights_hidden_local(model,features,adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 6968, 2400, 380)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_weight_list[0]),len(model_weight_list[1]),len(model_weight_list[2]),len(model_weight_list[3]),len(model_weight_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_list = get_weights_hidden_local(model,features,adj)\n",
    "layer1_hidden_weights = model_weight_list[2][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6965, 2400])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_hidden_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_test_emb(tokenize_test_sentences, word_id_map, vocab_length, word_doc_freq, word_list, train_size):\n",
    "    test_size = len(tokenize_test_sentences)\n",
    "    #print(\"tokenize_test_sentences...\",tokenize_test_sentences,len(tokenize_test_sentences))\n",
    "    test_emb = [[0]*vocab_length for _ in range(test_size)]\n",
    "    #print(\"test_emb...\",test_emb)\n",
    "    #print(len(test_emb))\n",
    "    tokenized_test_edge = [[0]*vocab_length+[1] for _ in range(test_size)]\n",
    "    #print(\"tokenized_test_edge...\",tokenized_test_edge)\n",
    "    row=[]\n",
    "    col=[]\n",
    "    weight=[]\n",
    "    for i in range(test_size):\n",
    "        tokenized_test_sample = tokenize_test_sentences[i]\n",
    "        word_freq_list = [0]*vocab_length\n",
    "        for word in tokenized_test_sample:\n",
    "            if word in word_id_map:\n",
    "                #print(\"word and idmap \", word,  word_id_map)\n",
    "                word_freq_list[word_id_map[word]]+=1\n",
    "          \n",
    "        #print(\"tokenized_test_sample ...\",tokenized_test_sample)\n",
    "        for word in tokenized_test_sample:\n",
    "            if word in word_id_map:\n",
    "                j = word_id_map[word]\n",
    "                freq = word_freq_list[j]   \n",
    "                idf = log(1.0 * train_size / word_doc_freq[word_list[j]])\n",
    "                w = freq * idf\n",
    "                test_emb[i][j] = w/len(tokenized_test_sample)\n",
    "                tokenized_test_edge[i][j] = w\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "                weight.append(w)\n",
    "         \n",
    "        #norm_item_temp = (1/norm_item[train_size:]).tolist()+[np.sqrt(np.sum(tokenized_test_edge[i]))]\n",
    "        #tokenized_test_edge[i] = tokenized_test_edge[i]/np.array(norm_item_temp)\n",
    "\n",
    "    tokenized_test_edge = np.array(tokenized_test_edge)\n",
    "    #adj = sp.csr_matrix((weight, (row, col)), shape=(vocab_length+1, vocab_length+1))\n",
    "    #adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)  \n",
    " \n",
    "    #print(\"test_emb\",test_emb)\n",
    "    #print(\"tokenized_test_edge\",tokenized_test_edge)\n",
    "    return test_emb, tokenized_test_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize_sentences[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Test input\n",
    "test_emb, tokenized_test_edge = get_test_emb(tokenize_sentences[train_size:], word_id_map, vocab_length, word_doc_freq, word_list, train_size)\n",
    "#len(test_emb[0]),tokenized_test_edge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb = torch.tensor(test_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1500, 1968])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_emb = torch.reshape(torch.tensor(test_emb),(3,35))\n",
    "#tokenized_test_edge = torch.FloatTensor(np.array(tokenized_test_edge.todense()))\n",
    "tokenized_test_edge = torch.tensor(tokenized_test_edge)\n",
    "#test_emb.shape,tokenized_test_edge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_edge.shape,len(tokenized_test_edge),tokenized_test_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rachna Mam Test function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5000, 1968]), torch.Size([6968, 6968]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1500, 1968]), (1500, 1969))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emb.shape,tokenized_test_edge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "where() received an invalid combination of arguments - got (numpy.ndarray, Tensor, Tensor), but expected one of:\n * (Tensor condition)\n * (Tensor condition, Tensor input, Tensor other, *, Tensor out)\n * (Tensor condition, Number self, Tensor other)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Tensor!, Tensor)\n * (Tensor condition, Tensor input, Number other)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, Tensor, !Tensor!)\n * (Tensor condition, Number self, Number other)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Tensor!, !Tensor!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22140/496835311.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenized_test_edge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22140/459439326.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(\"Adding x \",x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22140/459439326.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(\"Adding x \",x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22140/2020795017.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, adj)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mzero_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m9e15\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#print(\"Adj shape\",adj.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: where() received an invalid combination of arguments - got (numpy.ndarray, Tensor, Tensor), but expected one of:\n * (Tensor condition)\n * (Tensor condition, Tensor input, Tensor other, *, Tensor out)\n * (Tensor condition, Number self, Tensor other)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Tensor!, Tensor)\n * (Tensor condition, Tensor input, Number other)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, Tensor, !Tensor!)\n * (Tensor condition, Number self, Number other)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Tensor!, !Tensor!)\n"
     ]
    }
   ],
   "source": [
    "model(test_emb,tokenized_test_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_result = []\n",
    "for ind in range(len(test_emb)):\n",
    "    print(ind)\n",
    "    tokenized_test_edge_temp = tokenized_test_edge[ind]\n",
    "    print(tokenized_test_edge_temp.shape)\n",
    "    layer1_out = []\n",
    "    for head in range(model.nheads):\n",
    "        t = torch.reshape(torch.tensor(test_emb[ind]),(1,test_emb[ind].shape[0]))\n",
    "        print(t.shape)\n",
    "        #test_emb=test_emb[ind]\n",
    "        print(\"model.attentions[head].W\",model.attentions[head].W.shape)\n",
    "        x = torch.mm(t, model.attentions[head].W)#(1*7)*(7*16)=(1*16)\n",
    "        x = torch.vstack((x,model.attentions[head].W))#(1*16)+(7*16)=(8*16)\n",
    "        print(\"model.attentions[head].a[:model.attentions[head].out_features, :]\",model.attentions[head].a[:model.attentions[head].out_features, :].shape)\n",
    "        Wh1 = torch.matmul(x, model.attentions[head].a[:model.attentions[head].out_features, :])#(8*16)*(16*1)\n",
    "        Wh2 = torch.matmul(x, model.attentions[head].a[model.attentions[head].out_features:, :])\n",
    "        e = Wh1 + Wh2.T#8*8\n",
    "        print(e.shape)\n",
    "        e = model.attentions[head].leakyrelu(e)\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(tokenized_test_edge_temp > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        #x = torch.vstack((x,model.attentions[head].W))\n",
    "        h_prime = torch.matmul(attention, x)\n",
    "        layer1_out.append(F.elu(h_prime))\n",
    "        \n",
    "    #combine all the heads output\n",
    "    layer1_out = torch.cat(layer1_out,dim=1)\n",
    "    print(\"layer1_out.shape\",layer1_out.shape) #8*32\n",
    "    print(\"model.out_att.W\",model.out_att.W.shape) #32*2\n",
    "    \n",
    "    x = torch.mm(layer1_out, model.out_att.W)\n",
    "    print(\"x\",x.shape)#8*2\n",
    "    \n",
    "    Wh1 = torch.matmul(x, model.out_att.a[:model.out_att.out_features, :])\n",
    "    Wh2 = torch.matmul(x, model.out_att.a[model.out_att.out_features:, :])\n",
    "    e = Wh1 + Wh2.T\n",
    "    e = model.out_att.leakyrelu(e)\n",
    "    zero_vec = -9e15*torch.ones_like(e)\n",
    "    attention = torch.where(tokenized_test_edge_temp > 0, e, zero_vec)\n",
    "    attention = F.softmax(attention, dim=1)\n",
    "    \n",
    "    h_prime = torch.matmul(attention, x)#(1*36)*(36*3)\n",
    "    print(F.softmax(F.elu(h_prime),dim=1))\n",
    "    predict_temp = torch.argmax(F.softmax(F.elu(h_prime),dim=1)).cpu().detach().tolist()\n",
    "    test_result.append(predict_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_result = []\n",
    "for ind in range(len(test_emb)):\n",
    "    print(ind)\n",
    "    tokenized_test_edge_temp = tokenized_test_edge[ind]\n",
    "    layer1_out = []\n",
    "    for head in range(model.nheads):\n",
    "        t = torch.reshape(torch.tensor(test_emb[ind]),(1,test_emb[ind].shape[0]))\n",
    "        #test_emb=test_emb[ind]\n",
    "        print(\"model.attentions[head].W\",model.attentions[head].W.shape)\n",
    "        #x = torch.mm(test_emb[ind], model.attentions[head].W)#(1*35)*(35*32)=(1*32)\n",
    "        #x = torch.mm(t, model.attentions[head].W)#(1*35)*(35*32)=(1*32)\n",
    "        x = torch.mm(t, model.attentions[head].W)#(1*7)*(7*16)=(1*16)\n",
    "        #x = torch.vstack((x,model.attentions[head].W))#(1*16)+(7*16)=(8*16)\n",
    "        print(\"model.attentions[head].a[:model.attentions[head].out_features, :]\",model.attentions[head].a[:model.attentions[head].out_features, :].shape)\n",
    "        Wh1 = torch.matmul(x, model.attentions[head].a[:model.attentions[head].out_features, :])#(1*32)*(35*32)\n",
    "        Wh2 = torch.matmul(x, model.attentions[head].a[model.attentions[head].out_features:, :])\n",
    "        e = Wh1 + Wh2.T\n",
    "        e = model.attentions[head].leakyrelu(e)\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(tokenized_test_edge_temp > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        x = torch.vstack((x,model.attentions[head].W))\n",
    "        h_prime = torch.matmul(attention, x)\n",
    "        layer1_out.append(F.elu(h_prime))\n",
    "        \n",
    "    #combine all the heads output\n",
    "    layer1_out = torch.cat(layer1_out,dim=1)\n",
    "    print(\"layer1_out.shape\",layer1_out.shape) #1*384\n",
    "    print(\"model.out_att.W\",model.out_att.W.shape) #384*3\n",
    "    x = torch.mm(layer1_out, model.out_att.W)#1*3\n",
    "    print(\"x\",x.shape)\n",
    "    Wh1 = torch.matmul(x, model.out_att.a[:model.out_att.out_features, :])\n",
    "    Wh2 = torch.matmul(x, model.out_att.a[model.out_att.out_features:, :])\n",
    "    e = Wh1 + Wh2.T\n",
    "    e = model.out_att.leakyrelu(e)\n",
    "    zero_vec = -9e15*torch.ones_like(e)\n",
    "    attention = torch.where(tokenized_test_edge_temp > 0, e, zero_vec)\n",
    "    attention = F.softmax(attention, dim=1)\n",
    "    layer1_hidden_weights = model.hid[len(train_sentences):] #last layer after train size\n",
    "    print(\"layer1_hidden_weights\",layer1_hidden_weights.shape)\n",
    "    test_hidden_temp = torch.cat((layer1_hidden_weights,layer1_out))\n",
    "    x = torch.mm(test_hidden_temp, model.out_att.W)\n",
    "    print(\"x 2nd layer\",x.shape)\n",
    "    print(\"attention\",attention.shape)\n",
    "    h_prime = torch.matmul(attention, x)#(1*36)*(36*3)\n",
    "    print(F.softmax(F.elu(h_prime),dim=1))\n",
    "    predict_temp = torch.argmax(F.softmax(F.elu(h_prime),dim=1)).cpu().detach().tolist()\n",
    "    test_result.append(predict_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels\n",
    "#1 : 'IwantToSetUpAtransfer'\n",
    "#0 : 'Contributions'\n",
    "#2 : 'IwantToSetUpAwithdrawal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l_comb=test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = encode_onehot(l_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels,d = encode_onehot(l_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = torch.Tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = torch.LongTensor(np.where(labels)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[5601]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels[idx_test].cpu(),test_result,digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# for ind in range(len(test_emb)):\n",
    "\n",
    "# Â Â Â  tokenized_test_edge_temp = tokenized_test_edge[ind]\n",
    "# Â Â Â  layer1_out = []\n",
    "# Â Â Â  for head in range(model.nheads):\n",
    "# Â Â Â Â Â Â Â  test_emb = torch.reshape(test_emb[ind],(1,test_emb[ind].shape[0]))\n",
    "# Â Â Â Â Â Â Â  #print(test_emb.shape)\n",
    "# Â Â Â Â Â Â Â  x = torch.mm(test_emb, model.attentions[head].W)\n",
    "# Â Â Â Â Â Â Â  x = torch.vstack((x,model.attentions[head].W))\n",
    "\n",
    "# Â Â Â Â Â Â Â  Wh1 = torch.matmul(x, model.attentions[head].a[:model.attentions[head].out_features, :])\n",
    "# Â Â Â Â Â Â Â  Wh2 = torch.matmul(x, model.attentions[head].a[model.attentions[head].out_features:, :])\n",
    "# Â Â Â Â Â Â Â  e = Wh1 + Wh2.T\n",
    "# Â Â Â Â Â Â Â  e = model.attentions[head].leakyrelu(e)\n",
    "# Â Â Â Â Â Â Â  zero_vec = -9e15*torch.ones_like(e)\n",
    "# Â Â Â Â Â Â Â  attention = torch.where(tokenized_test_edge_temp > 0, e, zero_vec)\n",
    "# Â Â Â Â Â Â Â  attention = F.softmax(attention, dim=1)\n",
    "\n",
    "# Â Â Â Â Â Â Â  h_prime = torch.matmul(attention, x)\n",
    "# Â Â Â Â Â Â Â  layer1_out.append(F.elu(h_prime))\n",
    "\n",
    "# Â Â Â  #combine all the heads output\n",
    "# Â Â Â  layer1_out = torch.cat(layer1_out,dim=1)\n",
    "\n",
    "# Â Â Â  #print(\"layer1_out = \",layer1_out.shape)\n",
    "# Â Â Â  x = torch.mm(layer1_out, model.out_att.W)\n",
    "# Â Â Â  Wh1 = torch.matmul(x, model.out_att.a[:model.out_att.out_features, :])\n",
    "# Â Â Â  Wh2 = torch.matmul(x, model.out_att.a[model.out_att.out_features:, :])\n",
    "# Â Â Â  e = Wh1 + Wh2.T\n",
    "# Â Â Â  e = model.out_att.leakyrelu(e)\n",
    "# Â Â Â  zero_vec = -9e15*torch.ones_like(e)\n",
    "# Â Â Â  attention = torch.where(tokenized_test_edge_temp > 0, e, zero_vec)\n",
    "# Â Â Â  attention = F.softmax(attention, dim=1)\n",
    "\n",
    "# Â Â Â  h_prime = torch.matmul(attention, x)\n",
    "# Â Â Â  print(F.softmax(F.elu(h_prime),dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function Implementation Anup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape,adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_edge.shape,test_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(features, adj).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3500, 3501, 3502,  ..., 4997, 4998, 4999])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=idx_test-3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    #print(output[idx_test],labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 1.1551 accuracy= 0.7567\n"
     ]
    }
   ],
   "source": [
    "compute_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
